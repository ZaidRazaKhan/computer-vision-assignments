{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import shutil\n",
    "import hashlib\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from typing import *\n",
    "\n",
    "from numba import jit\n",
    "import requests\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette('muted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float_array(img: Image.Image) -> np.ndarray:\n",
    "    return np.array(img).astype(np.float32) / 255.\n",
    "\n",
    "def to_image(values: np.ndarray) -> Image.Image:\n",
    "    return Image.fromarray(np.uint8(values * 255.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma(values: np.ndarray, coeff: float=2.2) -> np.ndarray:\n",
    "    return values**(1./coeff)\n",
    "def gleam(values: np.ndarray) -> np.ndarray:\n",
    "    return np.sum(gamma(values), axis=2) / values.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_integral(img: np.ndarray) -> np.ndarray:\n",
    "    integral = np.cumsum(np.cumsum(img, axis=0), axis=1)\n",
    "    return np.pad(integral, (1, 1), 'constant', constant_values=(0, 0))[:-1, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Box:\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        self.coords_x = [x, x + width, x,          x + width]\n",
    "        self.coords_y = [y, y,         y + height, y + height]\n",
    "        self.coeffs   = [1, -1,        -1,         1]\n",
    "    \n",
    "    def __call__(self, integral_image: np.ndarray) -> float:\n",
    "        return np.sum(np.multiply(integral_image[self.coords_y, self.coords_x], self.coeffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'dataset'\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.mkdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, path: str):\n",
    "    print('Downloading file ...')\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(path, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "    print('Download completed.')\n",
    "    \n",
    "def md5(path: str, chunk_size: int=65536) -> str:\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(path, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b''):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "def untar(file_path: str, dest_path: str):\n",
    "    print('Extracting file.')\n",
    "    with tarfile.open(file_path, 'r:gz') as f:\n",
    "        f.extractall(dest_path)\n",
    "    print('Extraction completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_url = 'https://www.dropbox.com/s/ubjjoo0b2wz4vgz/faces_aligned_small_mirrored_co_aligned_cropped_cleaned.tar.gz?dl=1'\n",
    "faces_md5 = 'ab853c17ca6630c191457ff1fb16c1a4'\n",
    "\n",
    "faces_archive = os.path.join(dataset_path, 'faces_aligned_small_mirrored_co_aligned_cropped_cleaned.tar.gz')\n",
    "faces_dir = os.path.join(dataset_path, 'faces_aligned_small_mirrored_co_aligned_cropped_cleaned')\n",
    "\n",
    "if not os.path.exists(faces_archive) or md5(faces_archive) != faces_md5:\n",
    "    download_file(faces_url, faces_archive)\n",
    "\n",
    "if not os.path.exists(faces_dir):\n",
    "    untar(faces_archive, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37921"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_image_files = glob.glob(os.path.join(faces_dir, '**', '*.png'), recursive=True)\n",
    "len(face_image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_face(path: str, resize: bool=True) -> Image.Image:\n",
    "    CROP_TOP = 50\n",
    "    \n",
    "    img = Image.open(path)\n",
    "    img = to_image(gamma(to_float_array(img)[CROP_TOP:, :]))\n",
    "    min_size = np.min(img.size)\n",
    "    img = ImageOps.fit(img, (min_size, min_size), Image.ANTIALIAS)\n",
    "    if resize:\n",
    "        img = img.resize((WINDOW_SIZE, WINDOW_SIZE), Image.ANTIALIAS)\n",
    "    return img.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_images(files: Iterable[str], open_fun: Callable, resize: bool=False) -> Image.Image:\n",
    "    images = [open_fun(f, resize) for f in files]\n",
    "    sizes = [img.size for img in images]\n",
    "    collage_width = np.sum([size[0] for size in sizes])\n",
    "    collage_height = np.max([size[1] for size in sizes])\n",
    "\n",
    "    result = Image.new('L', (collage_width, collage_height))\n",
    "    x_offset = 0\n",
    "    for img, size in zip(images, sizes):\n",
    "        result.paste(im=img, box=(x_offset, 0))\n",
    "        x_offset += size[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backgrounds_url = 'http://dags.stanford.edu/data/iccv09Data.tar.gz'\n",
    "backgrounds_md5 = 'f469cf0ab459d94990edcf756694f4d5'\n",
    "\n",
    "backgrounds_archive = os.path.join(dataset_path, 'iccv09Data.tar.gz')\n",
    "backgrounds_dir = os.path.join(dataset_path, 'iccv09Data')\n",
    "\n",
    "if not os.path.exists(backgrounds_archive) or md5(backgrounds_archive) != faces_md5:\n",
    "    download_file(backgrounds_url, backgrounds_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(backgrounds_dir):\n",
    "    untar(backgrounds_archive, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backgrounds_dir = os.path.join(dataset_path, 'iccv09Data')\n",
    "background_image_files = glob.glob(os.path.join(backgrounds_dir, '**', '*.jpg'), recursive=True)\n",
    "len(background_image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(img: Image.Image) -> Image.Image:\n",
    "    max_allowed_size = np.min(img.size)\n",
    "    size = random.randint(WINDOW_SIZE, max_allowed_size)\n",
    "    max_width = img.size[0] - size - 1\n",
    "    max_height = img.size[1] - size - 1\n",
    "    left = 0 if (max_width <= 1)  else random.randint(0, max_width)\n",
    "    top  = 0 if (max_height <= 1) else random.randint(0, max_height)\n",
    "    return img.crop((left,top,left+size,top+size))\n",
    "\n",
    "def open_background(path: str, resize: bool=True) -> Image.Image:\n",
    "    img = Image.open(path)\n",
    "    img = to_image(gleam(to_float_array(img)))\n",
    "    img = random_crop(img)\n",
    "    if resize:\n",
    "        img = img.resize((WINDOW_SIZE, WINDOW_SIZE), Image.ANTIALIAS)\n",
    "    return img.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature:\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "    \n",
    "    def __call__(self, integral_image: np.ndarray) -> float:\n",
    "        try:\n",
    "            return np.sum(np.multiply(integral_image[self.coords_y, self.coords_x], self.coeffs))\n",
    "        except IndexError as e:\n",
    "            raise IndexError(str(e) + ' in ' + str(self))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}(x={self.x}, y={self.y}, width={self.width}, height={self.height})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature2h(Feature):\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        super().__init__(x, y, width, height)\n",
    "        hw = width // 2\n",
    "        self.coords_x = [x,      x + hw,     x,          x + hw,\n",
    "                         x + hw, x + width,  x + hw,     x + width]\n",
    "        self.coords_y = [y,      y,          y + height, y + height,\n",
    "                         y,      y,          y + height, y + height]\n",
    "        self.coeffs   = [1,     -1,         -1,          1,\n",
    "                         -1,     1,          1,         -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature2v(Feature):\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        super().__init__(x, y, width, height)\n",
    "        hh = height // 2        \n",
    "        self.coords_x = [x,      x + width,  x,          x + width,\n",
    "                         x,      x + width,  x,          x + width]\n",
    "        self.coords_y = [y,      y,          y + hh,     y + hh,\n",
    "                         y + hh, y + hh,     y + height, y + height]\n",
    "        self.coeffs   = [-1,     1,          1,         -1,\n",
    "                         1,     -1,         -1,          1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature3h(Feature):\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        super().__init__(x, y, width, height)\n",
    "        tw = width // 3\n",
    "        self.coords_x = [x,        x + tw,    x,          x + tw,\n",
    "                         x + tw,   x + 2*tw,  x + tw,     x + 2*tw,\n",
    "                         x + 2*tw, x + width, x + 2*tw,   x + width]\n",
    "        self.coords_y = [y,        y,         y + height, y + height,\n",
    "                         y,        y,         y + height, y + height,\n",
    "                         y,        y,         y + height, y + height]\n",
    "        self.coeffs   = [-1,       1,         1,         -1,\n",
    "                          1,      -1,        -1,          1,\n",
    "                         -1,       1,         1,         -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature3v(Feature):\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        super().__init__(x, y, width, height)\n",
    "        th = height // 3\n",
    "        self.coords_x = [x,        x + width,  x,          x + width,\n",
    "                         x,        x + width,  x,          x + width,\n",
    "                         x,        x + width,  x,          x + width]\n",
    "        self.coords_y = [y,        y,          y + th,     y + th,\n",
    "                         y + th,   y + th,     y + 2*th,   y + 2*th,\n",
    "                         y + 2*th, y + 2*th,   y + height, y + height]\n",
    "        self.coeffs   = [-1,        1,         1,         -1,\n",
    "                          1,       -1,        -1,          1,\n",
    "                         -1,        1,         1,         -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature4(Feature):\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        super().__init__(x, y, width, height)\n",
    "        hw = width // 2\n",
    "        hh = height // 2\n",
    "        self.coords_x = [x,      x + hw,     x,          x + hw,     # upper row\n",
    "                         x + hw, x + width,  x + hw,     x + width,\n",
    "                         x,      x + hw,     x,          x + hw,     # lower row\n",
    "                         x + hw, x + width,  x + hw,     x + width]\n",
    "        self.coords_y = [y,      y,          y + hh,     y + hh,     # upper row\n",
    "                         y,      y,          y + hh,     y + hh,\n",
    "                         y + hh, y + hh,     y + height, y + height, # lower row\n",
    "                         y + hh, y + hh,     y + height, y + height]\n",
    "        self.coeffs   = [1,     -1,         -1,          1,          # upper row\n",
    "                         -1,     1,          1,         -1,\n",
    "                         -1,     1,          1,         -1,          # lower row\n",
    "                          1,    -1,         -1,          1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Size = NamedTuple('Size', [('height', int), ('width', int)])\n",
    "Location = NamedTuple('Location', [('top', int), ('left', int)])\n",
    "WINDOW_SIZE = 15\n",
    "def possible_position(size: int, window_size: int = WINDOW_SIZE) -> Iterable[int]:\n",
    "    return range(0, window_size - size + 1)\n",
    "\n",
    "def possible_locations(base_shape: Size, window_size: int = WINDOW_SIZE) -> Iterable[Location]:\n",
    "    return (Location(left=x, top=y)\n",
    "            for x in possible_position(base_shape.width, window_size) \n",
    "            for y in possible_position(base_shape.height, window_size))\n",
    "\n",
    "def possible_shapes(base_shape: Size, window_size: int = WINDOW_SIZE) -> Iterable[Size]:\n",
    "    base_height = base_shape.height\n",
    "    base_width = base_shape.width\n",
    "    return (Size(height=height, width=width)\n",
    "            for width in range(base_width, window_size + 1, base_width)\n",
    "            for height in range(base_height, window_size + 1, base_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Location(top=0, left=0),\n",
       "  Location(top=1, left=0),\n",
       "  Location(top=0, left=1),\n",
       "  Location(top=1, left=1)],\n",
       " [Size(height=4, width=4)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(possible_locations(Size(4, 4), 5)), list(possible_shapes(Size(4, 4), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Location(top=0, left=0),\n",
       " Location(top=1, left=0),\n",
       " Location(top=2, left=0),\n",
       " Location(top=3, left=0),\n",
       " Location(top=4, left=0),\n",
       " Location(top=0, left=1),\n",
       " Location(top=1, left=1),\n",
       " Location(top=2, left=1),\n",
       " Location(top=3, left=1),\n",
       " Location(top=4, left=1),\n",
       " Location(top=0, left=2),\n",
       " Location(top=1, left=2),\n",
       " Location(top=2, left=2),\n",
       " Location(top=3, left=2),\n",
       " Location(top=4, left=2),\n",
       " Location(top=0, left=3),\n",
       " Location(top=1, left=3),\n",
       " Location(top=2, left=3),\n",
       " Location(top=3, left=3),\n",
       " Location(top=4, left=3)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(possible_locations(Size(height=1, width=2), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Size(height=1, width=2),\n",
       " Size(height=2, width=2),\n",
       " Size(height=3, width=2),\n",
       " Size(height=4, width=2),\n",
       " Size(height=5, width=2),\n",
       " Size(height=1, width=4),\n",
       " Size(height=2, width=4),\n",
       " Size(height=3, width=4),\n",
       " Size(height=4, width=4),\n",
       " Size(height=5, width=4)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(possible_shapes(Size(height=1, width=2), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature2h features: 6720\n",
      "Number of feature2v features: 6720\n",
      "Number of feature3h features: 4200\n",
      "Number of feature3v features: 4200\n",
      "Number of feature4 features:  3136\n",
      "Total number of features:     24976\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature2h = list(Feature2h(location.left, location.top, shape.width, shape.height)\n",
    "                 for shape in possible_shapes(Size(height=1, width=2), WINDOW_SIZE)\n",
    "                 for location in possible_locations(shape, WINDOW_SIZE))\n",
    "\n",
    "feature2v = list(Feature2v(location.left, location.top, shape.width, shape.height)\n",
    "                 for shape in possible_shapes(Size(height=2, width=1), WINDOW_SIZE)\n",
    "                 for location in possible_locations(shape, WINDOW_SIZE))\n",
    "\n",
    "feature3h = list(Feature3h(location.left, location.top, shape.width, shape.height)\n",
    "                 for shape in possible_shapes(Size(height=1, width=3), WINDOW_SIZE)\n",
    "                 for location in possible_locations(shape, WINDOW_SIZE))\n",
    "\n",
    "feature3v = list(Feature3v(location.left, location.top, shape.width, shape.height)\n",
    "                 for shape in possible_shapes(Size(height=3, width=1), WINDOW_SIZE)\n",
    "                 for location in possible_locations(shape, WINDOW_SIZE))\n",
    "\n",
    "feature4  = list(Feature4(location.left, location.top, shape.width, shape.height)\n",
    "                 for shape in possible_shapes(Size(height=2, width=2), WINDOW_SIZE)\n",
    "                 for location in possible_locations(shape, WINDOW_SIZE))\n",
    "\n",
    "features = feature2h + feature2v + feature3h + feature3v + feature4\n",
    "\n",
    "print(f'Number of feature2h features: {len(feature2h)}')\n",
    "print(f'Number of feature2v features: {len(feature2v)}')\n",
    "print(f'Number of feature3h features: {len(feature3h)}')\n",
    "print(f'Number of feature3v features: {len(feature3v)}')\n",
    "print(f'Number of feature4 features:  {len(feature4)}')\n",
    "print(f'Total number of features:     {len(features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(p: int, n: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    xs = []\n",
    "    xs.extend([to_float_array(open_face(f)) for f in random.sample(face_image_files, p)])\n",
    "    xs.extend([to_float_array(open_background(f)) for f in np.random.choice(background_image_files, n, replace=True)])\n",
    "\n",
    "    ys = np.hstack([np.ones((p,)), np.zeros((n,))])\n",
    "    return np.array(xs), ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_samples, _ = sample_data(100, 100)\n",
    "\n",
    "sample_mean = image_samples.mean()\n",
    "sample_std = image_samples.std()\n",
    "def normalize(im: np.ndarray, mean: float = sample_mean, std: float = sample_std) -> np.ndarray:\n",
    "    return (im - mean) / std\n",
    "\n",
    "def sample_data_normalized(p: int, n: int, mean: float = sample_mean, std: float = sample_std) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    xs, ys = sample_data(p, n)\n",
    "    xs = normalize(xs, mean, std)\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictionStats = NamedTuple('PredictionStats', [('tn', int), ('fp', int), ('fn', int), ('tp', int)])\n",
    "\n",
    "def prediction_stats(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[np.ndarray, PredictionStats]:\n",
    "    c = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = c.ravel()\n",
    "    return c, PredictionStats(tn=tn, fp=fp, fn=fn, tp=tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThresholdPolarity = NamedTuple('ThresholdPolarity', [('threshold', float), ('polarity', float)])\n",
    "\n",
    "ClassifierResult = NamedTuple('ClassifierResult', [('threshold', float), ('polarity', int), \n",
    "                                                   ('classification_error', float),\n",
    "                                                   ('classifier', Callable[[np.ndarray], float])])\n",
    "\n",
    "WeakClassifier = NamedTuple('WeakClassifier', [('threshold', float), ('polarity', int), \n",
    "                                               ('alpha', float), \n",
    "                                               ('classifier', Callable[[np.ndarray], float])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_classifier(x: np.ndarray, f: Feature, polarity: float, theta: float) -> float:\n",
    "    # return 1. if (polarity * f(x)) < (polarity * theta) else 0.\n",
    "    return (np.sign((polarity * theta) - (polarity * f(x))) + 1) // 2\n",
    "\n",
    "def run_weak_classifier(x: np.ndarray, c: WeakClassifier) -> float:\n",
    "    return weak_classifier(x=x, f=c.classifier, polarity=c.polarity, theta=c.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strong_classifier(x: np.ndarray, weak_classifiers: List[WeakClassifier]) -> int:\n",
    "    sum_hypotheses = 0.\n",
    "    sum_alphas = 0.\n",
    "    for c in weak_classifiers:\n",
    "        sum_hypotheses += c.alpha * run_weak_classifier(x, c)\n",
    "        sum_alphas += c.alpha\n",
    "    return 1 if (sum_hypotheses >= .5*sum_alphas) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_weights(w: np.ndarray) -> np.ndarray:\n",
    "    return w / w.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_running_sums(ys: np.ndarray, ws: np.ndarray) -> Tuple[float, float, List[float], List[float]]:\n",
    "    s_minus, s_plus = 0., 0.\n",
    "    t_minus, t_plus = 0., 0.\n",
    "    s_minuses, s_pluses = [], []\n",
    "    \n",
    "    for y, w in zip(ys, ws):\n",
    "        if y < .5:\n",
    "            s_minus += w\n",
    "            t_minus += w\n",
    "        else:\n",
    "            s_plus += w\n",
    "            t_plus += w\n",
    "        s_minuses.append(s_minus)\n",
    "        s_pluses.append(s_plus)\n",
    "    return t_minus, t_plus, s_minuses, s_pluses\n",
    "\n",
    "\n",
    "def find_best_threshold(zs: np.ndarray, t_minus: float, t_plus: float, s_minuses: List[float], s_pluses: List[float]) -> ThresholdPolarity:\n",
    "    min_e = float('inf')\n",
    "    min_z, polarity = 0, 0\n",
    "    for z, s_m, s_p in zip(zs, s_minuses, s_pluses):\n",
    "        error_1 = s_p + (t_minus - s_m)\n",
    "        error_2 = s_m + (t_plus - s_p)\n",
    "        if error_1 < min_e:\n",
    "            min_e = error_1\n",
    "            min_z = z\n",
    "            polarity = -1\n",
    "        elif error_2 < min_e:\n",
    "            min_e = error_2\n",
    "            min_z = z\n",
    "            polarity = 1\n",
    "    return ThresholdPolarity(threshold=min_z, polarity=polarity)\n",
    "\n",
    "\n",
    "def determine_threshold_polarity(ys: np.ndarray, ws: np.ndarray, zs: np.ndarray) -> ThresholdPolarity:  \n",
    "    # Sort according to score\n",
    "    p = np.argsort(zs)\n",
    "    zs, ys, ws = zs[p], ys[p], ws[p]\n",
    "    \n",
    "    # Determine the best threshold: build running sums\n",
    "    t_minus, t_plus, s_minuses, s_pluses = build_running_sums(ys, ws)\n",
    "    \n",
    "    # Determine the best threshold: select optimal threshold.\n",
    "    return find_best_threshold(zs, t_minus, t_plus, s_minuses, s_pluses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature(f: Feature, xis: np.ndarray, ys: np.ndarray, ws: np.ndarray, parallel: Optional[Parallel] = None) -> ClassifierResult:   \n",
    "    if parallel is None:\n",
    "        parallel = Parallel(n_jobs=-1, backend='threading')\n",
    "    \n",
    "    # Determine all feature values\n",
    "    zs = np.array(parallel(delayed(f)(x) for x in xis))\n",
    "    \n",
    "    # Determine the best threshold\n",
    "    result = determine_threshold_polarity(ys, ws, zs)\n",
    "            \n",
    "    # Determine the classification error\n",
    "    classification_error = 0.\n",
    "    for x, y, w in zip(xis, ys, ws):\n",
    "        h = weak_classifier(x, f, result.polarity, result.threshold)\n",
    "        classification_error += w * np.abs(h - y)\n",
    "            \n",
    "    return ClassifierResult(threshold=result.threshold, polarity=result.polarity, \n",
    "                            classification_error=classification_error, classifier=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS_EVERY     = 2000\n",
    "KEEP_PROBABILITY = 1./4.\n",
    "\n",
    "def build_weak_classifiers(prefix: str, num_features: int, xis: np.ndarray, ys: np.ndarray, features: List[Feature], ws: Optional[np.ndarray] = None) -> Tuple[List[WeakClassifier], List[float]]:\n",
    "    if ws is None:\n",
    "        m = len(ys[ys < .5])  # number of negative example\n",
    "        l = len(ys[ys > .5])  # number of positive examples\n",
    "\n",
    "        # Initialize the weights\n",
    "        ws = np.zeros_like(ys)\n",
    "        ws[ys < .5] = 1./(2.*m)\n",
    "        ws[ys > .5] = 1./(2.*l)\n",
    "    \n",
    "    # Keep track of the history of the example weights.\n",
    "    w_history = [ws]\n",
    "\n",
    "    total_start_time = datetime.now()\n",
    "    with Parallel(n_jobs=-1, backend='threading') as parallel:\n",
    "        weak_classifiers = []  # type: List[WeakClassifier]\n",
    "        for t in range(num_features):\n",
    "            print(f'Building weak classifier {t+1}/{num_features} ...')\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Normalize the weights\n",
    "            ws = normalize_weights(ws)\n",
    "            \n",
    "            status_counter = STATUS_EVERY\n",
    "\n",
    "            # Select best weak classifier for this round\n",
    "            best = ClassifierResult(polarity=0, threshold=0, classification_error=float('inf'), classifier=None)\n",
    "            for i, f in enumerate(features):\n",
    "                status_counter -= 1\n",
    "                improved = False\n",
    "\n",
    "                # Python runs singlethreaded. To speed things up,\n",
    "                # we're only anticipating every other feature, give or take.\n",
    "                if KEEP_PROBABILITY < 1.:\n",
    "                    skip_probability = np.random.random()\n",
    "                    if skip_probability > KEEP_PROBABILITY:\n",
    "                        continue\n",
    "\n",
    "                result = apply_feature(f, xis, ys, ws, parallel)\n",
    "                if result.classification_error < best.classification_error:\n",
    "                    improved = True\n",
    "                    best = result\n",
    "\n",
    "                # Print status every couple of iterations.\n",
    "                if improved or status_counter == 0:\n",
    "                    current_time = datetime.now()\n",
    "                    duration = current_time - start_time\n",
    "                    total_duration = current_time - total_start_time\n",
    "                    status_counter = STATUS_EVERY\n",
    "                    if improved:\n",
    "                        print(f't={t+1}/{num_features} {total_duration.total_seconds():.2f}s ({duration.total_seconds():.2f}s in this stage) {i+1}/{len(features)} {100*i/len(features):.2f}% evaluated. Classification error improved to {best.classification_error:.5f} using {str(best.classifier)} ...')\n",
    "                    else:\n",
    "                        print(f't={t+1}/{num_features} {total_duration.total_seconds():.2f}s ({duration.total_seconds():.2f}s in this stage) {i+1}/{len(features)} {100*i/len(features):.2f}% evaluated.')\n",
    "\n",
    "            # After the best classifier was found, determine alpha\n",
    "            beta = best.classification_error / (1 - best.classification_error)\n",
    "            alpha = np.log(1. / beta)\n",
    "            \n",
    "            # Build the weak classifier\n",
    "            classifier = WeakClassifier(threshold=best.threshold, polarity=best.polarity, classifier=best.classifier, alpha=alpha)\n",
    "            \n",
    "            # Update the weights for misclassified examples\n",
    "            for i, (x, y) in enumerate(zip(xis, ys)):\n",
    "                h = run_weak_classifier(x, classifier)\n",
    "                e = np.abs(h - y)\n",
    "                ws[i] = ws[i] * np.power(beta, 1-e)\n",
    "                \n",
    "            # Register this weak classifier           \n",
    "            weak_classifiers.append(classifier)\n",
    "            w_history.append(ws)\n",
    "        \n",
    "            pickle.dump(classifier, open(f'{prefix}-weak-learner-{t+1}-of-{num_features}.pickle', 'wb'))\n",
    "    \n",
    "    print(f'Done building {num_features} weak classifiers.')\n",
    "    return weak_classifiers, w_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(18151210)\n",
    "np.random.seed(18151210)\n",
    "xs, ys = sample_data_normalized(1000, 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1700, 15, 15), (1700, 16, 16))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xis = np.array([to_integral(x) for x in xs])\n",
    "xs.shape, xis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building weak classifier 1/2 ...\n",
      "t=1/2 0.18s (0.18s in this stage) 1/24976 0.00% evaluated. Classification error improved to 0.38907 using Feature2h(x=0, y=0, width=2, height=1) ...\n",
      "t=1/2 0.37s (0.37s in this stage) 5/24976 0.02% evaluated. Classification error improved to 0.34200 using Feature2h(x=0, y=4, width=2, height=1) ...\n",
      "t=1/2 0.55s (0.55s in this stage) 6/24976 0.02% evaluated. Classification error improved to 0.31786 using Feature2h(x=0, y=5, width=2, height=1) ...\n",
      "t=1/2 1.87s (1.87s in this stage) 21/24976 0.08% evaluated. Classification error improved to 0.26979 using Feature2h(x=1, y=5, width=2, height=1) ...\n",
      "t=1/2 2.04s (2.04s in this stage) 22/24976 0.08% evaluated. Classification error improved to 0.21286 using Feature2h(x=1, y=6, width=2, height=1) ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-ec56149588d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_xis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_integral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_xs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mWINDOW_SIZE\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWINDOW_SIZE\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mweak_classifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_weak_classifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1st'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-93b267e4451a>\u001b[0m in \u001b[0;36mbuild_weak_classifiers\u001b[0;34m(prefix, num_features, xis, ys, features, ws)\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_error\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0mimproved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-df6694e3a071>\u001b[0m in \u001b[0;36mapply_feature\u001b[0;34m(f, xis, ys, ws, parallel)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Determine all feature values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mzs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Determine the best threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \"\"\"\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random.seed(1337)\n",
    "np.random.seed(1337)\n",
    "test_xs, test_ys = sample_data_normalized(1000, 1000)\n",
    "test_xis = np.array([to_integral(x) for x in test_xs])\n",
    "assert xis.shape[1:3] == (WINDOW_SIZE+1, WINDOW_SIZE+1), xis.shape\n",
    "weak_classifiers, w_history = build_weak_classifiers('1st', 2, xis, ys, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1338)\n",
    "np.random.seed(1338)\n",
    "test_xs, test_ys = sample_data_normalized(1000, 1000)\n",
    "test_xis = np.array([to_integral(x) for x in test_xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_classifiers_2, w_history_2 = build_weak_classifiers('2nd', 10, xis, ys, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_classifiers_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1339)\n",
    "np.random.seed(1339)\n",
    "test_xs, test_ys = sample_data_normalized(5000, 5000)\n",
    "test_xis = np.array([to_integral(x) for x in test_xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_classifiers_3, w_history_3 = build_weak_classifiers('3rd', 25, xis, ys, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_classifiers_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_candidates(image: Image.Image, candidates: List[Tuple[int, int]]) -> Image.Image:\n",
    "    canvas = to_float_array(image.copy())\n",
    "    for row, col in candidates:\n",
    "        canvas[row-HALF_WINDOW-1:row+HALF_WINDOW, col-HALF_WINDOW-1, :] = [1., 0., 0.]\n",
    "        canvas[row-HALF_WINDOW-1:row+HALF_WINDOW, col+HALF_WINDOW-1, :] = [1., 0., 0.]\n",
    "        canvas[row-HALF_WINDOW-1, col-HALF_WINDOW-1:col+HALF_WINDOW, :] = [1., 0., 0.]\n",
    "        canvas[row+HALF_WINDOW-1, col-HALF_WINDOW-1:col+HALF_WINDOW, :] = [1., 0., 0.]\n",
    "    return to_image(canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_of_images = glob.glob('./images/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_of_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = Image.open(label_of_images[0])\n",
    "target_size = (384, 288)\n",
    "thumbnail_image = original_image.copy()\n",
    "thumbnail_image.thumbnail(target_size, Image.ANTIALIAS)\n",
    "thumbnail_image\n",
    "original = to_float_array(thumbnail_image)\n",
    "grayscale = gleam(original)\n",
    "integral = to_integral(grayscale)\n",
    "rows, cols = integral.shape[0:2]\n",
    "HALF_WINDOW = WINDOW_SIZE // 2\n",
    "\n",
    "face_positions_1 = []\n",
    "face_positions_2 = []\n",
    "face_positions_3 = []\n",
    "\n",
    "normalized_integral = to_integral(normalize(grayscale))\n",
    "\n",
    "for row in range(HALF_WINDOW + 1, rows - HALF_WINDOW):\n",
    "    for col in range(HALF_WINDOW + 1, cols - HALF_WINDOW):\n",
    "        window = normalized_integral[row-HALF_WINDOW-1:row+HALF_WINDOW+1, col-HALF_WINDOW-1:col+HALF_WINDOW+1]\n",
    "                \n",
    "        # First cascade stage\n",
    "        probably_face = strong_classifier(window, weak_classifiers)\n",
    "        if probably_face < .5:\n",
    "            continue\n",
    "        face_positions_1.append((row, col))\n",
    "            \n",
    "        # Second cascade stage\n",
    "        probably_face = strong_classifier(window, weak_classifiers_2)\n",
    "        if probably_face < .5:\n",
    "            continue\n",
    "        face_positions_2.append((row, col))\n",
    "            \n",
    "        # Third cascade stage \n",
    "        probably_face = strong_classifier(window, weak_classifiers_3)\n",
    "        if probably_face < .5:\n",
    "            continue\n",
    "        face_positions_3.append((row, col))\n",
    "    \n",
    "print(f'Found {len(face_positions_1)} candidates at stage 1, {len(face_positions_2)} at stage 2 and {len(face_positions_3)} at stage 3.')\n",
    "\n",
    "render_candidates(thumbnail_image, face_positions_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
